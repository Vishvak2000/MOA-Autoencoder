{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801797f-ed23-4c56-982d-1df73c77a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from wj_autoencoders import Autoencoder, VariationalAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5b78e19-0887-42b2-932c-ce3370ffe8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device mps with manual seed 37.\n"
     ]
    }
   ],
   "source": [
    "seed = 37\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Set device (with seed)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"Using device {device} with manual seed {seed}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a3ee464-f30a-4e4e-88af-bc8dd30f2ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 872 (Genes: 772, Cells: 100)\n",
      "Total MoA labels: 608\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/wjohns/Downloads/lish_moa_annotated.csv', low_memory=False)\n",
    "\n",
    "# Separate treated samples from control samples\n",
    "treated_df = df[df['cp_type'] == 'trt_cp'].reset_index(drop=True)\n",
    "control_df = df[df['cp_type'] == 'ctrl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "# Extract gene expression and cell viability features\n",
    "gene_cols = [col for col in df.columns if col.startswith('g-')]\n",
    "cell_cols = [col for col in df.columns if col.startswith('c-')]\n",
    "feature_cols = gene_cols + cell_cols\n",
    "print(f\"Total features: {len(feature_cols)} (Genes: {len(gene_cols)}, Cells: {len(cell_cols)})\")\n",
    "\n",
    "# Extract MoA columns (excluding metadata and feature columns)\n",
    "metadata_cols = ['sig_id', 'drug_id', 'training', 'cp_type', 'cp_time', 'cp_dose']\n",
    "moa_cols = [col for col in df.columns \n",
    "            if not col.startswith('g-') and \n",
    "               not col.startswith('c-') and \n",
    "               col not in metadata_cols]\n",
    "print(f\"Total MoA labels: {len(moa_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "957f6826-baca-422c-9bfa-609ad7f642e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels from treated samples\n",
    "X = treated_df[feature_cols].values\n",
    "y = treated_df[moa_cols].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "y_val_tensor = torch.FloatTensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9711366c-1158-4c51-9dd0-7ef63b17b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 1024 * 8\n",
    "num_workers = 10 # Set to number of performance cores on your apple silicon\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, X_train_tensor)  # Autoencoder input = output\n",
    "val_dataset = TensorDataset(X_val_tensor, X_val_tensor)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    persistent_workers=True, \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e10cd1d-cdd8-4cf0-b1bd-46a57eb96fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Variational Autoencoder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VariationalAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=872, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc_log_var): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "    (12): Linear(in_features=512, out_features=872, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model parameters\n",
    "input_dim = len(feature_cols)\n",
    "hidden_dims = [512, 256, 128]  # Decreasing hidden layer sizes\n",
    "latent_dim = 64  # Bottleneck dimension size\n",
    "\n",
    "# Choose variational or standard autoencoder\n",
    "use_vae = True  # Set to False for standard autoencoder\n",
    "\n",
    "if use_vae:\n",
    "    model = VariationalAutoencoder(input_dim, hidden_dims, latent_dim)\n",
    "    print(\"Using Variational Autoencoder\")\n",
    "else:\n",
    "    model = Autoencoder(input_dim, hidden_dims, latent_dim)\n",
    "    print(\"Using Standard Autoencoder\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b546533-0b1f-4805-9366-66b9605f4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def vae_loss(recon_x, x, mu, log_var):\n",
    "    # Reconstruction loss\n",
    "    MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL divergence\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    \n",
    "    return MSE + 0.1 * KLD  # Beta parameter to control KL divergence weight\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=20, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3395a88f-737a-40e1-9018-4b20f8afe4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer, device, use_vae):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_vae:\n",
    "            recon_batch, mu, log_var = model(data)\n",
    "            loss = vae_loss(recon_batch, data, mu, log_var)\n",
    "        else:\n",
    "            recon_batch = model(data)\n",
    "            loss = F.mse_loss(recon_batch, data)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    return train_loss / len(train_loader.dataset)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, device, use_vae):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in val_loader:\n",
    "            data = data.to(device)\n",
    "            \n",
    "            if use_vae:\n",
    "                recon_batch, mu, log_var = model(data)\n",
    "                loss = vae_loss(recon_batch, data, mu, log_var)\n",
    "            else:\n",
    "                recon_batch = model(data)\n",
    "                loss = F.mse_loss(recon_batch, data)\n",
    "                \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    return val_loss / len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84ce7201-5b30-4c1f-8429-1bf4dd48da9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/500, Train Loss: 1013.355367, Val Loss: 866.510753\n",
      "Epoch 2/500, Train Loss: 878.618541, Val Loss: 773.051417\n",
      "Epoch 3/500, Train Loss: 790.684973, Val Loss: 661.460313\n",
      "Epoch 4/500, Train Loss: 720.795889, Val Loss: 748.529130\n",
      "Epoch 5/500, Train Loss: 692.331867, Val Loss: 876.924242\n",
      "Epoch 6/500, Train Loss: 675.782006, Val Loss: 995.968426\n",
      "Epoch 7/500, Train Loss: 665.276715, Val Loss: 1021.887488\n",
      "Epoch 8/500, Train Loss: 657.726475, Val Loss: 1002.379961\n",
      "Epoch 9/500, Train Loss: 654.300093, Val Loss: 915.030890\n",
      "Epoch 10/500, Train Loss: 649.027594, Val Loss: 797.385973\n",
      "Epoch 11/500, Train Loss: 644.688872, Val Loss: 707.491642\n",
      "Epoch 12/500, Train Loss: 641.361307, Val Loss: 662.263734\n",
      "Epoch 13/500, Train Loss: 637.266596, Val Loss: 634.925709\n",
      "Epoch 14/500, Train Loss: 632.314733, Val Loss: 614.062805\n",
      "Epoch 15/500, Train Loss: 626.177005, Val Loss: 596.513587\n",
      "Epoch 16/500, Train Loss: 620.314990, Val Loss: 586.933871\n",
      "Epoch 17/500, Train Loss: 614.583284, Val Loss: 583.863930\n",
      "Epoch 18/500, Train Loss: 610.493071, Val Loss: 583.614663\n",
      "Epoch 19/500, Train Loss: 606.762881, Val Loss: 582.621652\n",
      "Epoch 20/500, Train Loss: 603.943821, Val Loss: 579.740714\n",
      "Epoch 21/500, Train Loss: 601.217334, Val Loss: 575.938221\n",
      "Epoch 22/500, Train Loss: 598.621108, Val Loss: 571.410997\n",
      "Epoch 23/500, Train Loss: 596.029538, Val Loss: 566.544086\n",
      "Epoch 24/500, Train Loss: 593.381874, Val Loss: 563.389150\n",
      "Epoch 25/500, Train Loss: 591.074852, Val Loss: 560.721652\n",
      "Epoch 26/500, Train Loss: 589.288679, Val Loss: 558.905425\n",
      "Epoch 27/500, Train Loss: 587.380897, Val Loss: 557.123216\n",
      "Epoch 28/500, Train Loss: 585.051572, Val Loss: 555.888954\n",
      "Epoch 29/500, Train Loss: 582.807181, Val Loss: 555.346676\n",
      "Epoch 30/500, Train Loss: 580.277301, Val Loss: 554.764223\n",
      "Epoch 31/500, Train Loss: 578.534976, Val Loss: 553.374340\n",
      "Epoch 32/500, Train Loss: 577.758188, Val Loss: 551.219550\n",
      "Epoch 33/500, Train Loss: 575.530246, Val Loss: 550.693206\n",
      "Epoch 34/500, Train Loss: 574.057132, Val Loss: 550.255181\n",
      "Epoch 35/500, Train Loss: 572.245038, Val Loss: 548.336852\n",
      "Epoch 36/500, Train Loss: 570.462030, Val Loss: 545.609922\n",
      "Epoch 37/500, Train Loss: 568.427629, Val Loss: 544.921652\n",
      "Epoch 38/500, Train Loss: 566.763687, Val Loss: 543.352542\n",
      "Epoch 39/500, Train Loss: 565.136726, Val Loss: 541.248680\n",
      "Epoch 40/500, Train Loss: 562.842389, Val Loss: 538.109238\n",
      "Epoch 41/500, Train Loss: 559.562582, Val Loss: 534.950929\n",
      "Epoch 42/500, Train Loss: 557.668671, Val Loss: 531.523558\n",
      "Epoch 43/500, Train Loss: 555.014653, Val Loss: 529.954839\n",
      "Epoch 44/500, Train Loss: 552.269040, Val Loss: 528.199169\n",
      "Epoch 45/500, Train Loss: 550.156377, Val Loss: 525.907185\n",
      "Epoch 46/500, Train Loss: 548.207117, Val Loss: 524.509335\n",
      "Epoch 47/500, Train Loss: 546.205333, Val Loss: 522.454594\n",
      "Epoch 48/500, Train Loss: 545.234614, Val Loss: 520.704203\n",
      "Epoch 49/500, Train Loss: 543.684937, Val Loss: 518.121652\n",
      "Epoch 50/500, Train Loss: 541.756391, Val Loss: 515.908895\n",
      "Epoch 51/500, Train Loss: 539.944836, Val Loss: 513.754448\n",
      "Epoch 52/500, Train Loss: 538.826759, Val Loss: 511.631916\n",
      "Epoch 53/500, Train Loss: 537.088246, Val Loss: 510.166422\n",
      "Epoch 54/500, Train Loss: 536.565662, Val Loss: 509.390029\n",
      "Epoch 55/500, Train Loss: 534.947927, Val Loss: 508.553617\n",
      "Epoch 56/500, Train Loss: 533.407367, Val Loss: 506.153128\n",
      "Epoch 57/500, Train Loss: 532.005450, Val Loss: 504.328788\n",
      "Epoch 58/500, Train Loss: 530.802586, Val Loss: 502.990469\n",
      "Epoch 59/500, Train Loss: 528.774845, Val Loss: 502.424731\n",
      "Epoch 60/500, Train Loss: 528.022046, Val Loss: 501.279423\n",
      "Epoch 61/500, Train Loss: 526.780234, Val Loss: 498.845112\n",
      "Epoch 62/500, Train Loss: 524.747434, Val Loss: 497.689785\n",
      "Epoch 63/500, Train Loss: 523.867148, Val Loss: 495.738074\n",
      "Epoch 64/500, Train Loss: 522.259428, Val Loss: 495.162023\n",
      "Epoch 65/500, Train Loss: 520.906750, Val Loss: 494.044966\n",
      "Epoch 66/500, Train Loss: 520.083798, Val Loss: 491.495992\n",
      "Epoch 67/500, Train Loss: 518.022963, Val Loss: 491.348729\n",
      "Epoch 68/500, Train Loss: 516.376106, Val Loss: 489.731818\n",
      "Epoch 69/500, Train Loss: 515.358539, Val Loss: 488.819013\n",
      "Epoch 70/500, Train Loss: 514.718929, Val Loss: 488.959091\n",
      "Epoch 71/500, Train Loss: 513.144364, Val Loss: 487.537732\n",
      "Epoch 72/500, Train Loss: 511.901818, Val Loss: 486.479814\n",
      "Epoch 73/500, Train Loss: 509.765068, Val Loss: 485.484018\n",
      "Epoch 74/500, Train Loss: 509.561959, Val Loss: 484.124829\n",
      "Epoch 75/500, Train Loss: 508.329783, Val Loss: 483.984066\n",
      "Epoch 76/500, Train Loss: 507.155094, Val Loss: 484.193353\n",
      "Epoch 77/500, Train Loss: 506.454808, Val Loss: 482.390176\n",
      "Epoch 78/500, Train Loss: 505.113097, Val Loss: 482.411193\n",
      "Epoch 79/500, Train Loss: 504.657232, Val Loss: 481.996774\n",
      "Epoch 80/500, Train Loss: 503.107042, Val Loss: 480.093353\n",
      "Epoch 81/500, Train Loss: 502.223334, Val Loss: 478.731769\n",
      "Epoch 82/500, Train Loss: 500.541862, Val Loss: 477.983138\n",
      "Epoch 83/500, Train Loss: 500.230312, Val Loss: 477.684164\n",
      "Epoch 84/500, Train Loss: 499.333187, Val Loss: 476.665640\n",
      "Epoch 85/500, Train Loss: 498.756220, Val Loss: 473.721017\n",
      "Epoch 86/500, Train Loss: 496.616818, Val Loss: 474.982600\n",
      "Epoch 87/500, Train Loss: 495.963228, Val Loss: 473.145552\n",
      "Epoch 88/500, Train Loss: 495.225754, Val Loss: 471.901564\n",
      "Epoch 89/500, Train Loss: 492.733936, Val Loss: 471.288514\n",
      "Epoch 90/500, Train Loss: 492.841576, Val Loss: 469.928592\n",
      "Epoch 91/500, Train Loss: 491.592786, Val Loss: 469.267204\n",
      "Epoch 92/500, Train Loss: 489.432474, Val Loss: 468.441153\n",
      "Epoch 93/500, Train Loss: 488.814147, Val Loss: 465.928788\n",
      "Epoch 94/500, Train Loss: 487.721959, Val Loss: 463.705963\n",
      "Epoch 95/500, Train Loss: 486.719741, Val Loss: 463.659189\n",
      "Epoch 96/500, Train Loss: 485.974477, Val Loss: 462.736706\n",
      "Epoch 97/500, Train Loss: 483.780503, Val Loss: 459.336852\n",
      "Epoch 98/500, Train Loss: 483.741904, Val Loss: 458.528348\n",
      "Epoch 99/500, Train Loss: 482.305721, Val Loss: 459.473900\n",
      "Epoch 100/500, Train Loss: 482.003428, Val Loss: 457.649511\n",
      "Epoch 101/500, Train Loss: 480.888907, Val Loss: 455.455425\n",
      "Epoch 102/500, Train Loss: 480.036717, Val Loss: 455.212512\n",
      "Epoch 103/500, Train Loss: 479.287927, Val Loss: 455.340518\n",
      "Epoch 104/500, Train Loss: 477.632681, Val Loss: 452.423803\n",
      "Epoch 105/500, Train Loss: 477.651006, Val Loss: 450.630450\n",
      "Epoch 106/500, Train Loss: 477.093898, Val Loss: 450.253226\n",
      "Epoch 107/500, Train Loss: 475.212617, Val Loss: 450.718964\n",
      "Epoch 108/500, Train Loss: 474.998576, Val Loss: 448.634066\n",
      "Epoch 109/500, Train Loss: 473.308818, Val Loss: 447.222190\n",
      "Epoch 110/500, Train Loss: 473.149649, Val Loss: 447.851026\n",
      "Epoch 111/500, Train Loss: 472.085093, Val Loss: 447.844135\n",
      "Epoch 112/500, Train Loss: 472.679089, Val Loss: 446.317400\n",
      "Epoch 113/500, Train Loss: 472.177751, Val Loss: 445.283333\n",
      "Epoch 114/500, Train Loss: 470.162469, Val Loss: 444.447898\n",
      "Epoch 115/500, Train Loss: 469.909939, Val Loss: 443.723412\n",
      "Epoch 116/500, Train Loss: 468.864716, Val Loss: 443.024976\n",
      "Epoch 117/500, Train Loss: 468.343690, Val Loss: 443.446188\n",
      "Epoch 118/500, Train Loss: 467.874151, Val Loss: 441.231329\n",
      "Epoch 119/500, Train Loss: 466.782868, Val Loss: 441.368817\n",
      "Epoch 120/500, Train Loss: 466.376760, Val Loss: 441.612757\n",
      "Epoch 121/500, Train Loss: 465.635357, Val Loss: 438.894037\n",
      "Epoch 122/500, Train Loss: 465.798590, Val Loss: 439.432111\n",
      "Epoch 123/500, Train Loss: 464.900163, Val Loss: 438.112903\n",
      "Epoch 124/500, Train Loss: 464.391736, Val Loss: 437.459824\n",
      "Epoch 125/500, Train Loss: 463.760437, Val Loss: 437.164614\n",
      "Epoch 126/500, Train Loss: 462.320404, Val Loss: 436.095894\n",
      "Epoch 127/500, Train Loss: 461.922490, Val Loss: 435.478641\n",
      "Epoch 128/500, Train Loss: 461.417046, Val Loss: 434.331525\n",
      "Epoch 129/500, Train Loss: 460.896301, Val Loss: 434.483138\n",
      "Epoch 130/500, Train Loss: 460.922771, Val Loss: 433.433187\n",
      "Epoch 131/500, Train Loss: 459.289259, Val Loss: 433.284604\n",
      "Epoch 132/500, Train Loss: 458.414284, Val Loss: 432.803324\n",
      "Epoch 133/500, Train Loss: 458.427824, Val Loss: 431.800196\n",
      "Epoch 134/500, Train Loss: 459.222516, Val Loss: 431.352004\n",
      "Epoch 135/500, Train Loss: 458.237187, Val Loss: 433.007625\n",
      "Epoch 136/500, Train Loss: 456.680690, Val Loss: 430.854350\n",
      "Epoch 137/500, Train Loss: 456.148116, Val Loss: 431.207918\n",
      "Epoch 138/500, Train Loss: 457.361282, Val Loss: 431.273412\n",
      "Epoch 139/500, Train Loss: 456.686257, Val Loss: 428.798583\n",
      "Epoch 140/500, Train Loss: 456.033974, Val Loss: 428.305572\n",
      "Epoch 141/500, Train Loss: 454.807566, Val Loss: 428.082209\n",
      "Epoch 142/500, Train Loss: 454.247599, Val Loss: 430.558651\n",
      "Epoch 143/500, Train Loss: 453.994611, Val Loss: 426.829961\n",
      "Epoch 144/500, Train Loss: 453.369623, Val Loss: 426.589394\n",
      "Epoch 145/500, Train Loss: 453.271979, Val Loss: 426.602933\n",
      "Epoch 146/500, Train Loss: 453.338350, Val Loss: 425.600293\n",
      "Epoch 147/500, Train Loss: 452.873350, Val Loss: 426.633773\n",
      "Epoch 148/500, Train Loss: 452.273470, Val Loss: 425.719110\n",
      "Epoch 149/500, Train Loss: 451.402515, Val Loss: 425.068133\n",
      "Epoch 150/500, Train Loss: 451.516828, Val Loss: 424.427468\n",
      "Epoch 151/500, Train Loss: 451.149527, Val Loss: 423.895308\n",
      "Epoch 152/500, Train Loss: 450.099636, Val Loss: 422.573558\n",
      "Epoch 153/500, Train Loss: 449.691065, Val Loss: 423.842180\n",
      "Epoch 154/500, Train Loss: 448.802824, Val Loss: 421.556012\n",
      "Epoch 155/500, Train Loss: 449.014702, Val Loss: 421.910655\n",
      "Epoch 156/500, Train Loss: 448.045638, Val Loss: 421.575073\n",
      "Epoch 157/500, Train Loss: 447.565809, Val Loss: 420.804594\n",
      "Epoch 158/500, Train Loss: 447.852373, Val Loss: 420.334457\n",
      "Epoch 159/500, Train Loss: 447.871004, Val Loss: 420.170137\n",
      "Epoch 160/500, Train Loss: 447.273775, Val Loss: 420.334653\n",
      "Epoch 161/500, Train Loss: 446.553099, Val Loss: 418.644379\n",
      "Epoch 162/500, Train Loss: 446.447084, Val Loss: 418.862561\n",
      "Epoch 163/500, Train Loss: 445.632430, Val Loss: 418.541349\n",
      "Epoch 164/500, Train Loss: 446.887374, Val Loss: 417.643304\n",
      "Epoch 165/500, Train Loss: 444.227575, Val Loss: 419.532845\n",
      "Epoch 166/500, Train Loss: 446.384832, Val Loss: 416.926393\n",
      "Epoch 167/500, Train Loss: 444.331904, Val Loss: 416.889834\n",
      "Epoch 168/500, Train Loss: 444.162218, Val Loss: 417.483871\n",
      "Epoch 169/500, Train Loss: 443.789400, Val Loss: 416.606598\n",
      "Epoch 170/500, Train Loss: 443.491427, Val Loss: 415.820283\n",
      "Epoch 171/500, Train Loss: 443.301407, Val Loss: 415.261241\n",
      "Epoch 172/500, Train Loss: 442.623081, Val Loss: 417.032160\n",
      "Epoch 173/500, Train Loss: 442.251503, Val Loss: 414.611975\n",
      "Epoch 174/500, Train Loss: 441.438322, Val Loss: 414.422630\n",
      "Epoch 175/500, Train Loss: 441.190399, Val Loss: 414.619599\n",
      "Epoch 176/500, Train Loss: 440.912438, Val Loss: 414.068622\n",
      "Epoch 177/500, Train Loss: 441.575536, Val Loss: 413.873509\n",
      "Epoch 178/500, Train Loss: 440.914724, Val Loss: 413.350684\n",
      "Epoch 179/500, Train Loss: 440.979481, Val Loss: 413.399756\n",
      "Epoch 180/500, Train Loss: 440.711773, Val Loss: 413.103324\n",
      "Epoch 181/500, Train Loss: 439.539149, Val Loss: 412.091447\n",
      "Epoch 182/500, Train Loss: 439.408650, Val Loss: 412.719013\n",
      "Epoch 183/500, Train Loss: 439.049525, Val Loss: 412.859873\n",
      "Epoch 184/500, Train Loss: 438.329441, Val Loss: 410.991789\n",
      "Epoch 185/500, Train Loss: 438.584794, Val Loss: 411.366667\n",
      "Epoch 186/500, Train Loss: 438.997971, Val Loss: 411.013441\n",
      "Epoch 187/500, Train Loss: 438.042131, Val Loss: 410.890274\n",
      "Epoch 188/500, Train Loss: 438.258933, Val Loss: 410.308211\n",
      "Epoch 189/500, Train Loss: 438.100100, Val Loss: 410.176491\n",
      "Epoch 190/500, Train Loss: 437.844246, Val Loss: 410.655083\n",
      "Epoch 191/500, Train Loss: 437.021539, Val Loss: 409.912243\n",
      "Epoch 192/500, Train Loss: 436.607854, Val Loss: 410.259531\n",
      "Epoch 193/500, Train Loss: 437.121535, Val Loss: 409.813661\n",
      "Epoch 194/500, Train Loss: 436.892659, Val Loss: 409.582405\n",
      "Epoch 195/500, Train Loss: 436.798669, Val Loss: 410.119355\n",
      "Epoch 196/500, Train Loss: 436.154886, Val Loss: 408.122972\n",
      "Epoch 197/500, Train Loss: 435.598035, Val Loss: 409.810826\n",
      "Epoch 198/500, Train Loss: 435.421818, Val Loss: 409.233578\n",
      "Epoch 199/500, Train Loss: 434.795987, Val Loss: 408.911437\n",
      "Epoch 200/500, Train Loss: 435.273965, Val Loss: 408.808065\n",
      "Epoch 201/500, Train Loss: 434.704178, Val Loss: 408.054863\n",
      "Epoch 202/500, Train Loss: 434.140814, Val Loss: 407.731965\n",
      "Epoch 203/500, Train Loss: 433.801248, Val Loss: 407.186535\n",
      "Epoch 204/500, Train Loss: 434.126711, Val Loss: 407.781525\n",
      "Epoch 205/500, Train Loss: 433.926651, Val Loss: 407.061486\n",
      "Epoch 206/500, Train Loss: 433.450023, Val Loss: 407.202224\n",
      "Epoch 207/500, Train Loss: 433.468513, Val Loss: 407.770772\n",
      "Epoch 208/500, Train Loss: 433.142451, Val Loss: 406.867498\n",
      "Epoch 209/500, Train Loss: 433.746920, Val Loss: 405.963930\n",
      "Epoch 210/500, Train Loss: 433.053845, Val Loss: 406.579765\n",
      "Epoch 211/500, Train Loss: 432.772010, Val Loss: 406.100806\n",
      "Epoch 212/500, Train Loss: 431.706946, Val Loss: 405.784922\n",
      "Epoch 213/500, Train Loss: 431.964132, Val Loss: 405.837561\n",
      "Epoch 214/500, Train Loss: 431.475143, Val Loss: 405.702248\n",
      "Epoch 215/500, Train Loss: 432.470310, Val Loss: 405.488710\n",
      "Epoch 216/500, Train Loss: 431.873344, Val Loss: 404.836852\n",
      "Epoch 217/500, Train Loss: 431.115388, Val Loss: 405.553812\n",
      "Epoch 218/500, Train Loss: 430.873326, Val Loss: 404.539174\n",
      "Epoch 219/500, Train Loss: 430.397517, Val Loss: 404.643377\n",
      "Epoch 220/500, Train Loss: 430.681026, Val Loss: 404.377297\n",
      "Epoch 221/500, Train Loss: 430.055470, Val Loss: 403.968377\n",
      "Epoch 222/500, Train Loss: 430.584769, Val Loss: 404.891422\n",
      "Epoch 223/500, Train Loss: 429.589438, Val Loss: 404.655010\n",
      "Epoch 224/500, Train Loss: 430.240242, Val Loss: 403.472043\n",
      "Epoch 225/500, Train Loss: 429.273879, Val Loss: 403.454692\n",
      "Epoch 226/500, Train Loss: 429.731547, Val Loss: 404.182698\n",
      "Epoch 227/500, Train Loss: 429.278193, Val Loss: 403.322067\n",
      "Epoch 228/500, Train Loss: 429.009887, Val Loss: 404.001344\n",
      "Epoch 229/500, Train Loss: 428.957551, Val Loss: 402.830010\n",
      "Epoch 230/500, Train Loss: 429.365889, Val Loss: 403.127688\n",
      "Epoch 231/500, Train Loss: 429.134990, Val Loss: 403.064174\n",
      "Epoch 232/500, Train Loss: 428.687790, Val Loss: 402.204619\n",
      "Epoch 233/500, Train Loss: 427.840666, Val Loss: 402.469330\n",
      "Epoch 234/500, Train Loss: 427.954887, Val Loss: 403.113368\n",
      "Epoch 235/500, Train Loss: 427.941835, Val Loss: 401.742717\n",
      "Epoch 236/500, Train Loss: 428.010938, Val Loss: 402.375049\n",
      "Epoch 237/500, Train Loss: 427.403725, Val Loss: 402.210435\n",
      "Epoch 238/500, Train Loss: 426.955626, Val Loss: 401.613123\n",
      "Epoch 239/500, Train Loss: 427.467438, Val Loss: 401.077761\n",
      "Epoch 240/500, Train Loss: 426.508836, Val Loss: 403.752737\n",
      "Epoch 241/500, Train Loss: 426.700555, Val Loss: 400.621530\n",
      "Epoch 242/500, Train Loss: 426.865559, Val Loss: 401.544721\n",
      "Epoch 243/500, Train Loss: 425.663838, Val Loss: 401.887170\n",
      "Epoch 244/500, Train Loss: 425.813120, Val Loss: 400.788734\n",
      "Epoch 245/500, Train Loss: 426.673804, Val Loss: 400.208040\n",
      "Epoch 246/500, Train Loss: 426.061159, Val Loss: 400.294013\n",
      "Epoch 247/500, Train Loss: 426.345799, Val Loss: 400.111608\n",
      "Epoch 248/500, Train Loss: 425.201300, Val Loss: 400.119941\n",
      "Epoch 249/500, Train Loss: 425.980392, Val Loss: 400.561217\n",
      "Epoch 250/500, Train Loss: 426.101383, Val Loss: 400.249927\n",
      "Epoch 251/500, Train Loss: 424.902393, Val Loss: 399.723998\n",
      "Epoch 252/500, Train Loss: 424.827926, Val Loss: 399.877517\n",
      "Epoch 253/500, Train Loss: 424.671298, Val Loss: 398.849340\n",
      "Epoch 254/500, Train Loss: 424.852929, Val Loss: 399.548485\n",
      "Epoch 255/500, Train Loss: 424.224734, Val Loss: 399.694990\n",
      "Epoch 256/500, Train Loss: 423.796158, Val Loss: 399.681794\n",
      "Epoch 257/500, Train Loss: 424.299158, Val Loss: 399.533431\n",
      "Epoch 258/500, Train Loss: 424.402423, Val Loss: 399.443524\n",
      "Epoch 259/500, Train Loss: 424.607182, Val Loss: 398.907796\n",
      "Epoch 260/500, Train Loss: 423.162231, Val Loss: 398.299829\n",
      "Epoch 261/500, Train Loss: 423.069169, Val Loss: 399.223412\n",
      "Epoch 262/500, Train Loss: 424.349086, Val Loss: 398.432918\n",
      "Epoch 263/500, Train Loss: 423.295510, Val Loss: 397.784848\n",
      "Epoch 264/500, Train Loss: 422.707753, Val Loss: 398.838563\n",
      "Epoch 265/500, Train Loss: 422.154525, Val Loss: 397.940885\n",
      "Epoch 266/500, Train Loss: 422.373729, Val Loss: 397.376711\n",
      "Epoch 267/500, Train Loss: 422.404550, Val Loss: 398.076417\n",
      "Epoch 268/500, Train Loss: 422.536772, Val Loss: 398.884848\n",
      "Epoch 269/500, Train Loss: 421.849300, Val Loss: 396.955841\n",
      "Epoch 270/500, Train Loss: 422.592646, Val Loss: 398.360850\n",
      "Epoch 271/500, Train Loss: 422.198239, Val Loss: 396.580767\n",
      "Epoch 272/500, Train Loss: 421.162218, Val Loss: 395.707307\n",
      "Epoch 273/500, Train Loss: 420.937020, Val Loss: 397.035020\n",
      "Epoch 274/500, Train Loss: 420.661992, Val Loss: 396.121530\n",
      "Epoch 275/500, Train Loss: 421.695410, Val Loss: 395.965494\n",
      "Epoch 276/500, Train Loss: 420.927702, Val Loss: 395.827028\n",
      "Epoch 277/500, Train Loss: 420.811006, Val Loss: 395.743597\n",
      "Epoch 278/500, Train Loss: 421.620429, Val Loss: 395.914883\n",
      "Epoch 279/500, Train Loss: 420.279330, Val Loss: 395.028519\n",
      "Epoch 280/500, Train Loss: 420.049757, Val Loss: 395.762830\n",
      "Epoch 281/500, Train Loss: 420.672734, Val Loss: 394.809335\n",
      "Epoch 282/500, Train Loss: 419.996047, Val Loss: 394.583749\n",
      "Epoch 283/500, Train Loss: 419.976090, Val Loss: 395.639101\n",
      "Epoch 284/500, Train Loss: 419.982500, Val Loss: 394.545161\n",
      "Epoch 285/500, Train Loss: 419.485243, Val Loss: 395.081745\n",
      "Epoch 286/500, Train Loss: 419.773843, Val Loss: 394.941764\n",
      "Epoch 287/500, Train Loss: 420.071009, Val Loss: 394.728079\n",
      "Epoch 288/500, Train Loss: 419.704050, Val Loss: 395.109580\n",
      "Epoch 289/500, Train Loss: 418.590452, Val Loss: 393.750538\n",
      "Epoch 290/500, Train Loss: 419.159432, Val Loss: 393.603275\n",
      "Epoch 291/500, Train Loss: 418.293292, Val Loss: 393.549071\n",
      "Epoch 292/500, Train Loss: 418.039308, Val Loss: 393.759262\n",
      "Epoch 293/500, Train Loss: 419.407904, Val Loss: 394.208651\n",
      "Epoch 294/500, Train Loss: 418.145812, Val Loss: 394.011779\n",
      "Epoch 295/500, Train Loss: 417.661259, Val Loss: 393.860362\n",
      "Epoch 296/500, Train Loss: 418.629015, Val Loss: 393.861608\n",
      "Epoch 297/500, Train Loss: 418.660593, Val Loss: 393.367229\n",
      "Epoch 298/500, Train Loss: 416.771857, Val Loss: 392.811461\n",
      "Epoch 299/500, Train Loss: 416.720236, Val Loss: 392.595846\n",
      "Epoch 300/500, Train Loss: 417.688682, Val Loss: 392.661681\n",
      "Epoch 301/500, Train Loss: 417.616812, Val Loss: 392.731500\n",
      "Epoch 302/500, Train Loss: 417.374853, Val Loss: 392.215640\n",
      "Epoch 303/500, Train Loss: 416.842921, Val Loss: 392.617595\n",
      "Epoch 304/500, Train Loss: 416.909377, Val Loss: 392.068475\n",
      "Epoch 305/500, Train Loss: 416.372464, Val Loss: 392.000758\n",
      "Epoch 306/500, Train Loss: 417.401818, Val Loss: 393.500000\n",
      "Epoch 307/500, Train Loss: 416.782428, Val Loss: 391.853812\n",
      "Epoch 308/500, Train Loss: 416.355942, Val Loss: 393.029521\n",
      "Epoch 309/500, Train Loss: 417.370020, Val Loss: 392.155938\n",
      "Epoch 310/500, Train Loss: 416.481027, Val Loss: 391.967278\n",
      "Epoch 311/500, Train Loss: 416.870558, Val Loss: 391.488221\n",
      "Epoch 312/500, Train Loss: 416.211034, Val Loss: 391.828519\n",
      "Epoch 313/500, Train Loss: 415.466735, Val Loss: 390.835288\n",
      "Epoch 314/500, Train Loss: 415.534120, Val Loss: 390.357796\n",
      "Epoch 315/500, Train Loss: 415.617826, Val Loss: 390.866251\n",
      "Epoch 316/500, Train Loss: 415.575268, Val Loss: 390.758553\n",
      "Epoch 317/500, Train Loss: 415.786290, Val Loss: 392.415225\n",
      "Epoch 318/500, Train Loss: 415.086835, Val Loss: 390.500880\n",
      "Epoch 319/500, Train Loss: 415.972473, Val Loss: 391.244746\n",
      "Epoch 320/500, Train Loss: 414.688035, Val Loss: 390.246359\n",
      "Epoch 321/500, Train Loss: 414.545956, Val Loss: 390.246481\n",
      "Epoch 322/500, Train Loss: 414.347852, Val Loss: 389.813294\n",
      "Epoch 323/500, Train Loss: 414.898714, Val Loss: 390.433333\n",
      "Epoch 324/500, Train Loss: 413.924806, Val Loss: 390.362390\n",
      "Epoch 325/500, Train Loss: 413.866500, Val Loss: 390.222043\n",
      "Epoch 326/500, Train Loss: 413.762361, Val Loss: 389.412732\n",
      "Epoch 327/500, Train Loss: 413.042687, Val Loss: 389.416300\n",
      "Epoch 328/500, Train Loss: 414.332447, Val Loss: 389.387879\n",
      "Epoch 329/500, Train Loss: 414.263376, Val Loss: 389.723631\n",
      "Epoch 330/500, Train Loss: 414.430091, Val Loss: 390.633260\n",
      "Epoch 331/500, Train Loss: 414.671042, Val Loss: 390.353617\n",
      "Epoch 332/500, Train Loss: 414.464383, Val Loss: 391.008944\n",
      "Epoch 333/500, Train Loss: 414.301400, Val Loss: 388.797972\n",
      "Epoch 334/500, Train Loss: 413.740706, Val Loss: 389.625929\n",
      "Epoch 335/500, Train Loss: 413.305171, Val Loss: 389.522605\n",
      "Epoch 336/500, Train Loss: 413.070667, Val Loss: 388.458431\n",
      "Epoch 337/500, Train Loss: 412.694457, Val Loss: 388.494575\n",
      "Epoch 338/500, Train Loss: 413.968397, Val Loss: 388.192913\n",
      "Epoch 339/500, Train Loss: 413.180831, Val Loss: 388.285606\n",
      "Epoch 340/500, Train Loss: 412.297869, Val Loss: 389.186584\n",
      "Epoch 341/500, Train Loss: 412.162854, Val Loss: 388.280279\n",
      "Epoch 342/500, Train Loss: 412.455590, Val Loss: 387.843377\n",
      "Epoch 343/500, Train Loss: 411.461743, Val Loss: 388.227444\n",
      "Epoch 344/500, Train Loss: 411.279305, Val Loss: 388.036070\n",
      "Epoch 345/500, Train Loss: 411.654000, Val Loss: 387.478055\n",
      "Epoch 346/500, Train Loss: 411.774160, Val Loss: 387.574756\n",
      "Epoch 347/500, Train Loss: 411.818387, Val Loss: 387.244086\n",
      "Epoch 348/500, Train Loss: 411.444127, Val Loss: 387.504570\n",
      "Epoch 349/500, Train Loss: 411.643423, Val Loss: 388.209897\n",
      "Epoch 350/500, Train Loss: 411.873020, Val Loss: 386.785606\n",
      "Epoch 351/500, Train Loss: 410.973261, Val Loss: 386.753446\n",
      "Epoch 352/500, Train Loss: 411.479353, Val Loss: 387.848998\n",
      "Epoch 353/500, Train Loss: 410.845151, Val Loss: 387.281476\n",
      "Epoch 354/500, Train Loss: 410.460716, Val Loss: 386.875269\n",
      "Epoch 355/500, Train Loss: 410.497428, Val Loss: 387.082478\n",
      "Epoch 356/500, Train Loss: 411.913575, Val Loss: 387.085997\n",
      "Epoch 357/500, Train Loss: 410.611960, Val Loss: 386.868133\n",
      "Epoch 358/500, Train Loss: 410.323257, Val Loss: 387.110826\n",
      "Epoch 359/500, Train Loss: 410.409108, Val Loss: 386.640323\n",
      "Epoch 360/500, Train Loss: 410.265221, Val Loss: 386.876026\n",
      "Epoch 361/500, Train Loss: 410.033632, Val Loss: 387.224976\n",
      "Epoch 362/500, Train Loss: 409.749420, Val Loss: 385.950000\n",
      "Epoch 363/500, Train Loss: 410.167553, Val Loss: 386.444966\n",
      "Epoch 364/500, Train Loss: 410.527313, Val Loss: 386.769330\n",
      "Epoch 365/500, Train Loss: 409.192935, Val Loss: 385.875147\n",
      "Epoch 366/500, Train Loss: 409.875464, Val Loss: 386.296041\n",
      "Epoch 367/500, Train Loss: 410.009709, Val Loss: 385.935288\n",
      "Epoch 368/500, Train Loss: 409.751858, Val Loss: 386.637732\n",
      "Epoch 369/500, Train Loss: 409.466857, Val Loss: 386.070357\n",
      "Epoch 370/500, Train Loss: 410.032868, Val Loss: 386.658944\n",
      "Epoch 371/500, Train Loss: 410.761756, Val Loss: 385.810508\n",
      "Epoch 372/500, Train Loss: 409.675496, Val Loss: 386.028788\n",
      "Epoch 373/500, Train Loss: 409.306167, Val Loss: 385.964052\n",
      "Epoch 374/500, Train Loss: 409.167889, Val Loss: 385.196774\n",
      "Epoch 375/500, Train Loss: 409.154110, Val Loss: 385.827395\n",
      "Epoch 376/500, Train Loss: 409.718287, Val Loss: 385.864907\n",
      "Epoch 377/500, Train Loss: 408.214945, Val Loss: 385.613196\n",
      "Epoch 378/500, Train Loss: 408.471440, Val Loss: 385.465714\n",
      "Epoch 379/500, Train Loss: 408.288147, Val Loss: 385.723314\n",
      "Epoch 380/500, Train Loss: 408.930629, Val Loss: 384.766251\n",
      "Epoch 381/500, Train Loss: 407.492924, Val Loss: 384.685777\n",
      "Epoch 382/500, Train Loss: 409.293231, Val Loss: 384.935044\n",
      "Epoch 383/500, Train Loss: 408.090935, Val Loss: 384.553495\n",
      "Epoch 384/500, Train Loss: 407.951251, Val Loss: 384.821603\n",
      "Epoch 385/500, Train Loss: 407.744623, Val Loss: 385.094746\n",
      "Epoch 386/500, Train Loss: 407.635608, Val Loss: 384.380181\n",
      "Epoch 387/500, Train Loss: 406.992478, Val Loss: 384.841373\n",
      "Epoch 388/500, Train Loss: 407.187668, Val Loss: 384.278397\n",
      "Epoch 389/500, Train Loss: 407.766138, Val Loss: 384.216373\n",
      "Epoch 390/500, Train Loss: 407.863073, Val Loss: 384.886804\n",
      "Epoch 391/500, Train Loss: 407.842951, Val Loss: 384.310484\n",
      "Epoch 392/500, Train Loss: 407.604396, Val Loss: 385.035313\n",
      "Epoch 393/500, Train Loss: 407.033778, Val Loss: 385.070723\n",
      "Epoch 394/500, Train Loss: 407.543066, Val Loss: 387.417449\n",
      "Epoch 395/500, Train Loss: 407.958895, Val Loss: 385.450342\n",
      "Epoch 396/500, Train Loss: 408.449614, Val Loss: 385.967742\n",
      "Epoch 397/500, Train Loss: 407.729426, Val Loss: 384.760093\n",
      "Epoch 398/500, Train Loss: 407.410672, Val Loss: 384.590909\n",
      "Epoch 399/500, Train Loss: 407.759434, Val Loss: 383.918915\n",
      "Epoch 400/500, Train Loss: 407.787567, Val Loss: 384.229692\n",
      "Epoch 401/500, Train Loss: 407.051938, Val Loss: 384.470088\n",
      "Epoch 402/500, Train Loss: 406.685377, Val Loss: 384.359873\n",
      "Epoch 403/500, Train Loss: 406.926407, Val Loss: 385.636290\n",
      "Epoch 404/500, Train Loss: 406.858716, Val Loss: 384.152077\n",
      "Epoch 405/500, Train Loss: 407.038489, Val Loss: 384.407429\n",
      "Epoch 406/500, Train Loss: 407.028652, Val Loss: 383.698094\n",
      "Epoch 407/500, Train Loss: 406.409481, Val Loss: 383.515543\n",
      "Epoch 408/500, Train Loss: 406.565283, Val Loss: 383.505572\n",
      "Epoch 409/500, Train Loss: 406.524234, Val Loss: 383.022043\n",
      "Epoch 410/500, Train Loss: 405.238451, Val Loss: 383.022287\n",
      "Epoch 411/500, Train Loss: 406.078812, Val Loss: 382.913441\n",
      "Epoch 412/500, Train Loss: 405.825274, Val Loss: 383.299682\n",
      "Epoch 413/500, Train Loss: 405.386934, Val Loss: 383.190860\n",
      "Epoch 414/500, Train Loss: 406.232084, Val Loss: 382.910777\n",
      "Epoch 415/500, Train Loss: 405.765594, Val Loss: 382.929472\n",
      "Epoch 416/500, Train Loss: 405.653725, Val Loss: 382.340176\n",
      "Epoch 417/500, Train Loss: 405.207551, Val Loss: 383.378812\n",
      "Epoch 418/500, Train Loss: 405.278688, Val Loss: 382.619795\n",
      "Epoch 419/500, Train Loss: 405.348628, Val Loss: 383.054863\n",
      "Epoch 420/500, Train Loss: 405.643802, Val Loss: 382.550318\n",
      "Epoch 421/500, Train Loss: 405.449730, Val Loss: 382.647776\n",
      "Epoch 422/500, Train Loss: 405.474489, Val Loss: 382.944135\n",
      "Epoch 423/500, Train Loss: 405.180672, Val Loss: 382.784018\n",
      "Epoch 424/500, Train Loss: 404.124811, Val Loss: 382.312830\n",
      "Epoch 425/500, Train Loss: 404.212562, Val Loss: 382.946848\n",
      "Epoch 426/500, Train Loss: 404.410159, Val Loss: 382.543500\n",
      "Epoch 427/500, Train Loss: 404.811861, Val Loss: 382.379619\n",
      "Epoch 428/500, Train Loss: 404.456164, Val Loss: 382.515274\n",
      "Epoch 429/500, Train Loss: 404.390416, Val Loss: 381.666593\n",
      "Epoch 430/500, Train Loss: 403.969839, Val Loss: 382.369257\n",
      "Epoch 431/500, Train Loss: 404.670651, Val Loss: 382.228030\n",
      "Epoch 432/500, Train Loss: 405.017635, Val Loss: 381.590225\n",
      "Epoch 433/500, Train Loss: 404.609590, Val Loss: 381.713099\n",
      "Epoch 434/500, Train Loss: 404.490798, Val Loss: 382.196383\n",
      "Epoch 435/500, Train Loss: 403.677543, Val Loss: 382.185166\n",
      "Epoch 436/500, Train Loss: 404.315656, Val Loss: 381.365054\n",
      "Epoch 437/500, Train Loss: 404.467591, Val Loss: 382.061975\n",
      "Epoch 438/500, Train Loss: 404.589957, Val Loss: 381.405816\n",
      "Epoch 439/500, Train Loss: 404.778872, Val Loss: 381.330474\n",
      "Epoch 440/500, Train Loss: 404.432187, Val Loss: 381.845283\n",
      "Epoch 441/500, Train Loss: 404.218745, Val Loss: 381.814174\n",
      "Epoch 442/500, Train Loss: 403.827657, Val Loss: 382.500929\n",
      "Epoch 443/500, Train Loss: 404.009728, Val Loss: 381.548485\n",
      "Epoch 444/500, Train Loss: 403.956635, Val Loss: 382.081452\n",
      "Epoch 445/500, Train Loss: 403.123197, Val Loss: 381.268133\n",
      "Epoch 446/500, Train Loss: 403.237969, Val Loss: 382.204839\n",
      "Epoch 447/500, Train Loss: 402.937906, Val Loss: 381.314247\n",
      "Epoch 448/500, Train Loss: 402.972925, Val Loss: 381.970283\n",
      "Epoch 449/500, Train Loss: 403.248778, Val Loss: 381.164638\n",
      "Epoch 450/500, Train Loss: 403.366647, Val Loss: 381.095503\n",
      "Epoch 451/500, Train Loss: 403.166936, Val Loss: 381.120259\n",
      "Epoch 452/500, Train Loss: 403.040982, Val Loss: 381.120161\n",
      "Epoch 453/500, Train Loss: 402.657232, Val Loss: 380.653519\n",
      "Epoch 454/500, Train Loss: 402.596367, Val Loss: 380.568304\n",
      "Epoch 455/500, Train Loss: 402.473762, Val Loss: 381.239907\n",
      "Epoch 456/500, Train Loss: 402.416997, Val Loss: 380.699658\n",
      "Epoch 457/500, Train Loss: 402.933458, Val Loss: 380.470846\n",
      "Epoch 458/500, Train Loss: 402.481724, Val Loss: 380.841007\n",
      "Epoch 459/500, Train Loss: 402.224630, Val Loss: 381.289761\n",
      "Epoch 460/500, Train Loss: 402.121535, Val Loss: 380.902908\n",
      "Epoch 461/500, Train Loss: 402.498020, Val Loss: 380.723803\n",
      "Epoch 462/500, Train Loss: 402.791966, Val Loss: 380.357429\n",
      "Epoch 463/500, Train Loss: 401.356712, Val Loss: 381.126613\n",
      "Epoch 464/500, Train Loss: 402.032177, Val Loss: 381.085899\n",
      "Epoch 465/500, Train Loss: 401.917131, Val Loss: 380.124145\n",
      "Epoch 466/500, Train Loss: 402.453744, Val Loss: 381.335557\n",
      "Epoch 467/500, Train Loss: 402.843825, Val Loss: 380.851320\n",
      "Epoch 468/500, Train Loss: 403.020335, Val Loss: 380.867302\n",
      "Epoch 469/500, Train Loss: 401.733154, Val Loss: 380.703910\n",
      "Epoch 470/500, Train Loss: 402.463600, Val Loss: 380.276564\n",
      "Epoch 471/500, Train Loss: 401.665145, Val Loss: 380.341691\n",
      "Epoch 472/500, Train Loss: 401.661369, Val Loss: 380.000684\n",
      "Epoch 473/500, Train Loss: 403.021435, Val Loss: 381.461755\n",
      "Epoch 474/500, Train Loss: 402.125073, Val Loss: 381.100391\n",
      "Epoch 475/500, Train Loss: 401.882901, Val Loss: 380.926808\n",
      "Epoch 476/500, Train Loss: 402.287854, Val Loss: 380.498289\n",
      "Epoch 477/500, Train Loss: 401.870014, Val Loss: 380.337146\n",
      "Epoch 478/500, Train Loss: 401.933287, Val Loss: 379.797556\n",
      "Epoch 479/500, Train Loss: 401.110531, Val Loss: 380.010508\n",
      "Epoch 480/500, Train Loss: 401.588154, Val Loss: 379.889027\n",
      "Epoch 481/500, Train Loss: 400.910025, Val Loss: 379.969550\n",
      "Epoch 482/500, Train Loss: 401.318228, Val Loss: 381.103446\n",
      "Epoch 483/500, Train Loss: 401.258946, Val Loss: 380.148045\n",
      "Epoch 484/500, Train Loss: 401.821772, Val Loss: 380.093964\n",
      "Epoch 485/500, Train Loss: 401.019639, Val Loss: 379.148705\n",
      "Epoch 486/500, Train Loss: 400.490486, Val Loss: 379.285753\n",
      "Epoch 487/500, Train Loss: 400.653407, Val Loss: 379.232209\n",
      "Epoch 488/500, Train Loss: 400.976261, Val Loss: 379.465127\n",
      "Epoch 489/500, Train Loss: 400.643563, Val Loss: 378.966862\n",
      "Epoch 490/500, Train Loss: 399.673431, Val Loss: 379.279277\n",
      "Epoch 491/500, Train Loss: 400.038673, Val Loss: 379.394477\n",
      "Epoch 492/500, Train Loss: 400.998772, Val Loss: 379.460068\n",
      "Epoch 493/500, Train Loss: 399.926211, Val Loss: 379.156476\n",
      "Epoch 494/500, Train Loss: 400.192306, Val Loss: 379.422385\n",
      "Epoch 495/500, Train Loss: 400.698080, Val Loss: 379.441667\n",
      "Epoch 496/500, Train Loss: 400.379222, Val Loss: 379.109506\n",
      "Epoch 497/500, Train Loss: 400.656542, Val Loss: 378.932307\n",
      "Epoch 498/500, Train Loss: 400.683091, Val Loss: 378.809946\n",
      "Epoch 499/500, Train Loss: 400.514860, Val Loss: 378.667571\n",
      "Epoch 500/500, Train Loss: 400.052739, Val Loss: 377.981745\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 500\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device, use_vae)\n",
    "    val_loss = validate(model, val_loader, device, use_vae)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_autoencoder_model.pt')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08607a35-854e-41b0-86a2-2ce721b0fe33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m### Plot loss curve using Matplotlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m      3\u001b[39m plt.plot(train_losses, label=\u001b[33m'\u001b[39m\u001b[33mTrain\u001b[39m\u001b[33m'\u001b[39m, marker=\u001b[33m'\u001b[39m\u001b[33mo\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m plt.plot(val_losses, label=\u001b[33m'\u001b[39m\u001b[33mValidation\u001b[39m\u001b[33m'\u001b[39m, marker=\u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "### Plot loss curve using Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train', marker='o')\n",
    "plt.plot(val_losses, label='Validation', marker='x')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75f458f1-9bda-47fb-a27d-9f33a8f6eeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape: (20457, 64)\n",
      "Validation embeddings shape: (5115, 64)\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_autoencoder_model.pt'))\n",
    "\n",
    "# Generate embeddings using the trained autoencoder\n",
    "def get_embeddings(model, data_loader, device, use_vae):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            data = data.to(device)\n",
    "            \n",
    "            if use_vae:\n",
    "                mu, _ = model.encode(data)\n",
    "                embeddings.append(mu.cpu().numpy())\n",
    "            else:\n",
    "                encoded = model.get_latent(data)\n",
    "                embeddings.append(encoded.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Get embeddings for train and validation sets\n",
    "train_embeddings = get_embeddings(model, train_loader, device, use_vae)\n",
    "val_embeddings = get_embeddings(model, val_loader, device, use_vae)\n",
    "\n",
    "print(f\"Train embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"Validation embeddings shape: {val_embeddings.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
